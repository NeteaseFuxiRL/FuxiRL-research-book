# 课题背景
解决实际的项目需求时，对于同一个游戏的不同场景（例如逆水寒的武学试炼， 流派挑战任务；流星蝴蝶剑中不同的关卡和武器）需要训练不同的
模型；对于不同游戏（即使场景类似）的不同场景，也需要训练不同的模型。目前的训练方式十分低效， 而显然相同场景的模型之间具有很多可迁移复用的知识， 利用这些知识实现快速的策略迁移， 提高效率。

# 问题定义
根据神经网络的输入（状态表示）， 输出（动作空间）的差别以及环境之间的差异， 我们可以把该问题分解成以下三种情况：

- state 输入维度（语义顺序）不变， action 空间（语义）不变 ， 游戏场景有变化（reward function 或 environment transition 不同）
- state 输入维度（语义顺序）发生变化， action 空间（语义）不变 ， 游戏场景有变化（reward function 或 environment transition 不同）
- state 输入维度（语义顺序）发生变化， action 空间（语义）也发生变化 ， 游戏场景有变化（reward function 或 environment transition 不同）

针对以上三种情形，能够设计合适的网络表示 和 相对应的训练算法（流程）。


# 问题挑战
- 如何设计网络结构能够应对可能的场景变化
- 如何学习得到更加高层抽象的概率或信息来辅助迁移

# 环境支持
- 流星蝴蝶剑的不同关卡boss对战和武器类型
- 其他通关类的游戏benchmark （例如超级玛丽）

# 评价指标
- 模型迁移的效率提升
- 模型迁移的效果提升

# 相关学术论文

1. 2019- Dota 2 with Large Scale Deep Reinforcement Learning.

2. 2019- Dynamics-aware Embeddings.

3. 2019- Neural Network Surgery with Sets.

4. 2019- Learning Action-Transferable Policy with Action Embedding.



# 联系我们

有任何问题，请联系chenyingfeng1@corp.netease.com， hzlvtangjie@corp.netease.com