# 课题背景
非集中式（Decentralized）学习是解决多智能体合作问题的一种重要方法。顾名思义，非集中式学习是指各个智能体独立自主地进行决策，凭借自己收到的反馈信号来学习如何与其他智能体进行合作。虽然一般情况下，集中式（Centralized）学习方法的效率更高，但非集中式学习却具有更好的灵活性（Flexibility）和伸缩性（Scalability）。尤其是在智能体数量比较多的情况下，巨大的通信开销和指数增长的决策空间使得集中式学习的可行性较弱。在现实生活中，也有很多非集中式决策的例子。比如，足球、篮球比赛中，同一个队的队员在球场上只需要根据自己的职责、位置和观察来采取行动与队友进行配合，参与到进攻或者防守中。

非集中式学习的主流方法是融合法。具体来讲，就是是利用一些启发式将每个agent的平均效用值函数和乐观效用值函数进行融合，并用融合后的值函数来指导agent进行动作选择。一方面，平均主义法存在relative over-generalization问题（也即最优动作被次优动作所掩盖），而乐观主义法能够避免该问题；另一方面，乐观主义法在随机环境中可能存在过估计（overestimation）的问题，而平均主义法则能够解决该问题。

然而，融合法的问题在于对agent的平均效用值函数和乐观效用值函数进行融合本身。如何恰到好处地将二者进行融合，使得所有agent能够学习到问题的最优策略，是很难进行控制的。一方面，融合后得到的值函数的含义并不明确，从理论上并不能保证一定能够学习到问题的最优策略。另一方面，融合之后的值函数也终究会带有平均效用和乐观效用的成分，很难把relative over-generalization问题和overestimation问题同时完全解决。

基于值函数分布（Distributional Value Function）的强化学习方法为我们提供了一个可能的解决方案。Agent的平均效用值函数和乐观效用值函数都是其值函数空间的一个点，显然包含在agent的值函数分布中。既然我们能够刻画agent的值函数分布，那么我们就不需要将平均效用值函数和乐观效用值函数进行融合，而是直接利用这个值函数分布来辅助agent进行决策。这其中可能的好处在于（1）不需要再去考虑怎么融合值函数，而是直接从值函数分布上评价agent策略的好坏，完全解决relative over-generalization、miscoordination、stochasticity等问题（2）为multi-agent coordination提供新的理论支撑。


# 问题定义

在合作类型的多智能体问题中，结合Distributional Reinforcement Learning理论，提出新型非集中式多智能体强化学习学习方法。

# 问题挑战

- 如何基于值函数分布来为agent选择动作，实现multi-agent coordination
 
- 算法收敛性证明

# 评价指标

- 算法在benchmark中的Reward、Coordination Ratio

- 收敛性证明

# 相关论文

- Reinforcement Learning of Coordination in Cooperative Multi-agent Systems, AAAI 2002.

- Lenient Learners in Cooperative Multiagent Systems, AAMAS 2006.

- Independent reinforcement learners in cooperative markov games-a survey regarding coordination problems, KER, 2010

- Deep Decentralized Multi-Task Multi-Agent Reinforcement Learning under Partial Observability, ICML 2017.

- Lenient Multi-Agent Deep Reinforcement Learning, AAMAS 2018.

- Distributional Perspective on Reinforcement Learning, ICML 2017.

# 联系我们

huyujing@corp.netease.com