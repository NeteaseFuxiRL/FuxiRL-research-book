# 游戏AI风格多样化

## 课题背景

游戏AI风格多样化技术对商业游戏非常重要。风格迥异的游戏AI能极大提升人类玩家的新鲜感，从而改善游戏体验与娱乐性。这要求算法能在赢得比赛的同时，主动控制AI行为。

强化学习在游戏AI领域取得了巨大的成功，但难以生成多样的AI行为。究其根因，是因为n种游戏风格实际构成了带n个目标的最优化问题。基于梯度下降的强化学习本质是单目标最优化算法，需要通过奖赏信号间接设定多种聚合参数才能搜索到n维目标空间中的不同解。人工设计奖赏信号费时费力，而且因为游戏过程的蝴蝶效应难以精确而稳定地控制实际行为。

## 问题定义

以逆水寒武学试炼场景为例，敌我双方在圆形的场地上博弈。我方职业为神像（法师职业，多远程攻击&魔法技能但血量较少），敌方为血河（肉盾近战职业）。要求在战胜对手的同时表现出不同风格，标准如下：

* 胜率。游戏过程中，若一方血量耗尽则判负；若游戏结束时双方都没死（血量>0），则根据游戏过程的累积伤害输出判定胜负。胜率要求最大化，且有约束条件（不得小于50%）。
* 激进。正常比赛中，对敌方的累积伤害总和最大化。
* 保守。从游戏第二帧开始可以根据前一帧计算相对对手的逃跑速度（远离对手为正；接近对手为负）。要求最大化平均逃跑速度。

可以看出，各目标存在一定冲突性，且后两个目标的冲突性较强（一直进攻则没时间逃跑，反之亦然）。此外，折中风格（一边跑一边打）往往有利于提高胜率。下面给出一些直观的例子：

* 激进：![](../.assets/游戏AI风格多样化/aggressive.gif)
* 折中：![](../.assets/游戏AI风格多样化/trade-off.gif)
* 保守：![](../.assets/游戏AI风格多样化/defensive.gif)

## 问题挑战

强化学习难以生成多样的AI行为。主要体现在：

* 需要人工调参。强化学习通过游戏环境的奖赏信号指导AI进行学习，为了得到不同的风格，需要根据专家经验设计多种对应的奖赏信号。
* 难以生成折中风格。很多游戏的折中风格对取胜至关重要（如逆水寒中的激进（猛烈进攻）和保守（一直逃跑）风格，用极端风格往往难以制胜（一直猛烈地进攻可能打不赢肉盾职业（如神像vs血河）；而一直逃跑也容易因为没有攻击输出被超时判负），最容易取胜的方法是一边跑一边打），但环境奖赏容易在游戏过程中发生蝴蝶效应，人工难以调校出精确的奖赏信号，稳定达到期望的折中风格。
* 游戏风格均匀分布。前文提到，n种游戏风格对应一个n维的目标空间。在目标空间中生成多样的风格，能为游戏运营方带来更多的选择余地。

## 评价指标

算法得到的解集多个包含不同风格的AI，映射到n(=3)维目标空间中。可以评价：

* 收敛性。在符合约束的前提下，每个目标都应当最优化（本例为最大化）。
* 分布广泛性。解集应当尽量扩大[Pareto最优面](https://zh.wikipedia.org/wiki/%E5%B8%95%E7%B4%AF%E6%89%98%E6%9C%80%E4%BC%98)的边界。
* 分布均匀性。解集在目标空间中保持均匀分布。

上述标准可以使用一些量化评价指标来计算。其中综合性能指标[hypervolume指标](https://www.omegaxyz.com/2019/03/09/hypervolume/)是一种不错的选择，简单来说是Pareto最优面与某个参考点所围成的超体积。超体积越大反应上述3种性能越好，其中对收敛性和分布广泛性相对更明显。

## 相关文献

* <https://arxiv.org/abs/1910.09022>
* <https://sites.google.com/view/emo-drl/>

## 联系方式

* shenruimin@corp.netease.com