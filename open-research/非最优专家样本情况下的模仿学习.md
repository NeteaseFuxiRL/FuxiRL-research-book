# 课题背景

模仿学习中我们一般都要求专家样本是最优并且一致性比较好的策略产生的，但是在实际的游戏场景下，这个要求往往不能满足，收集得到的玩家操作数据往往水平不一。传统的做法是在训练前对数据进行比较细致的清洗和预处理工作，这在大规模推广到多种多样的游戏环境中时，成本较高。是否存在某些方法，可以在提供了环境的情况下，让算法自行判断专家决策数据中的哪些行为是好的行为，哪些行为是不好的行为，从而让模型自行决定专家数据中的哪些决策是需要学习的，哪些是不需要学习的，达到在不需要完备的数据清洗的情况下就可以通过专家数据来达到加速强化学习的训练过程的目的。

# 问题定义

假设已经有一个数据集$D=\{d_1,d_2...d_n\}$，其中的$d_i=\{(s_1,a_1), (s_2,a_2)...(s_m,a_m)\}$表示一系列的state和action数据对，每一个数据对表示了某个策略$\pi_d$在遇到状态$s_m$时会采取的动作$a_m$。这些$（s_m,a_m)$数据对中有一些是好的行为，有一些是不好的行为。

在有数据集$D$和环境的情况下，需要从数据集中学习得到一个最优策略$\pi$, 且学习速度或者最终结果比不使用数据集而单独使用环境学习的情况更优.

### 目标

 1. 通过包含非最优示范样本的数据集来加速强化学习的训练。

# 问题挑战

该问题主要存在以下几个挑战：

- 如何定义和自动区分非最优示例并在学习过程中尽可能降低这些数据的负面影响。
- 如何自动区分比较好的示例并在学习过程中尽可能的让这部分数据来加速训练。

# 环境支持

- 可以提供逆水寒游戏的流派竞武对战模拟环境。
- 可以提供逆水寒中收集到的在流派竞武对战场景中的玩家数据(state和action数据对)。

# 评价指标

- 学习得到的策略的基本强度，如技能释放成功率、胜率等。
- 学习的速度，如与环境进行了一定步数的互动后的胜率。

# 相关学术论文

1. [2018- Deep q-learning from demonstrations](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16976). 

2. [2018- Policy Optimization with Demonstrations](http://proceedings.mlr.press/v80/kang18a.html).

# 联系我们

有任何问题，请联系wangmeng02@corp.netease.com