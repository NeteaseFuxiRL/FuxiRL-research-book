# 课题背景

深度强化学习（Deep Reinforcement Learning）自从提出以来，已经在游戏、机器人控制、推荐等场景中得到了广泛应用。尤其是在游戏领域中，已经有很多重量级的成果，比如：围棋中的AlphaGo、AlphaZero，德州扑克中的Libratus，MOBA游戏中的OpenFive等。尽管如此，想要在大型商业游戏中进行更加广泛且贴近用户的实际应用，还是有很大的困难。AlphaGo、OpenFive这类工作是由大量计算和存储资源来支撑起算法训练，并不适用于实际商业游戏。这是因为，商业游戏中的游戏AI不光是训练出一个强力模型那么简单，其中往往会经历若干个版本的方案和模型的迭代、测试、上线。对于计算资源不足的中小型游戏公司，是无法承受复杂的神经网络模型训练带来的时间成本。那么，如果不采用深度神经网络（或者说不采用神经网络），是否也能开发出有效的游戏AI模型呢？

在强化学习研究领域，已经有一些工作将简单函数（比如：线性函数、RBF）作为Function Apporximator，来获得跟深度强化学习方法相当的性能。以基于线性值函数近似（Linear Function Approximation）的强化学习方法为例，它不仅计算效率高，同时也能够较容易地进行收敛性分析（如：LSTD方法、GTD算法族）。目前这方面的工作在连续动作控制问题中已经有了较好的效果。然而，在实际的商业游戏中，agent的动作往往是离散的，有时甚至是离散加连续的。同时，整个问题的动作空间和状态空间也常常比较巨大（动辄几百维的状态空间、几百个离散动作），这也会对学习造成一定的困难。

本课题将研究如何在复杂游戏场景中，利用线性函数、RBF等简单函数模型作为Function Approximator，训练出不输于深度强化学习模型的Agent。这不仅能够大大减小训练成本，还有可能实现用户端上的在线学习。

# 问题定义

在大规模游戏场景中，使用线性函数、RBF函数作为Function Approximator的强化学习方法研究

# 问题挑战

- 大规模状态空间和动作空间问题

- 离散和连续结合的动作空间问题

# 评价指标

- benchmark中算法的reward等指标

- 训练时间、模型参数数量

- 实际游戏效果

# 相关论文

- Towards Generalization and Simplicity in Continuous Control, NIPS 2017

- Simple random search of static linear policies is competitive for reinforcement learning, NIPS 2018.

# 联系我们

huyujing@corp.netease.com