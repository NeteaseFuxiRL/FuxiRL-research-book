# 课题背景

一般情况下，神经网络对于相似的输入会产生相近的输出，这在相似输入对应不同真值时就会产生误判问题。为了增大关键输入间的区别，通常会在状态表示、网络结构上进行一定的优化。例如，使用one-hot状态表示替换数值的index，采取卷积、attention等结构。然而在强化学习问题中，这些方法都各自有一定的局限性，不能完全解决该问题。此外，基于temporal difference (TD) 的更新方式还会使得误差不断被传递。

在实际游戏应用中，这种问题也很常见。例如对技能CD进行刻画时，如果使用浮点数表示CD值，则神经网络很难区分0和0.1应当输出不同动作。

如何用一种更通用的方式学习出较好的特征表示，是本课题主要面对的问题。

# 问题定义

<img src="https://github.com/FuxiRL/FuxiRL-research-book/blob/master/.assets/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8A%B6%E6%80%81%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/image-20200113144153801.png" alt="问题示意图。左：环境示意；右：网络拟合V值示意" style="zoom:67%;" />

如图1左所示，一个矩形空间内，中间用一道墙隔成上下两个部分，二者不能互通。在下半部分存在一个终点（绿点），可以获得正reward。其他不同颜色线段表示不同轨迹。

如果使用表格（tabular based）来记录不同状态的V值，由于上半部分的轨迹无法获得reward，所以V值应为0；下半部分越靠近终点的状态V值越大。

但当使用神经网络来拟合V值时，由于靠近墙壁的两个状态十分相似，导致下半部分的非零V值会“泄露”到上半部分。如图1右所示，本应全为绿色的上半部分产生了非绿色区域。这种泄露现象会使得agent对于状态的价值产生误判，进而影响自身策略。

### 目标

1. 在不对输入状态进行额外处理的条件下，学习出更好的状态表示方法，减小神经网络中的V值泄露现象，进而提升agent的实际策略表现。

# 问题挑战

该问题主要存在以下几个挑战：

- 如何建模该问题，并设计解决方案。

# 环境支持

可提供包含对战模拟环境。

# 评价指标

- 对战过程数据统计，如技能释放成功率、胜率等

# 相关学术论文

1. 2018- Temporal Difference Learning with Neural Networks - Study of the Leakage Propagation Problem.

2. 2018- Expressiveness in Deep Reinforcement Learning.

3. 2018- Universal successor features approximators.

4. https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html

# 联系我们

有任何问题，请联系songyan@corp.netease.com